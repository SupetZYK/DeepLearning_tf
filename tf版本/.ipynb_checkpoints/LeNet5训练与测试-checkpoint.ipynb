{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.1\n",
      "Extracting /home/zyk/data/MNIST/train-images-idx3-ubyte.gz\n",
      "Extracting /home/zyk/data/MNIST/train-labels-idx1-ubyte.gz\n",
      "Extracting /home/zyk/data/MNIST/t10k-images-idx3-ubyte.gz\n",
      "Extracting /home/zyk/data/MNIST/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import numpy as np\n",
    "from LeNet5 import *\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "base_path='/home/zyk/data/MNIST'\n",
    "mnist=input_data.read_data_sets(base_path,one_hot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(mnist.train)\n",
    "# help(mnist.train.next_batch)\n",
    "# print(100*'-')\n",
    "# print(mnist.train.images.shape)\n",
    "# print(mnist.train.labels.shape)\n",
    "# print(mnist.train.num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# help(mnist.validation)\n",
    "# print(100*'-')\n",
    "# print(mnist.validation.images.shape)\n",
    "# print(mnist.validation.labels.shape)\n",
    "# print(mnist.validation.labels)\n",
    "# print(mnist.validation.num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(mnist.test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeNet-5 模型结构\n",
    "```\n",
    "INPUT: [28x28x1]           weights: 0\n",
    "CONV5-32: [28x28x32]       weights: (5*5*1+1)*32\n",
    "POOL2: [14x14x32]          weights: 0\n",
    "CONV5-64: [14x14x64]       weights: (5*5*32+1)*64\n",
    "POOL2: [7x7x64]          weights: 0\n",
    "FC: [1x1x512]              weights: (7*7*64+1)*512\n",
    "FC: [1x1x10]              weights: (1*1*512+1)*10\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文件IO\n",
    "MODEL_SAVE_PATH = base_path+'/models'\n",
    "MODEL_NAME = \"MNIST.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 网络超参数\n",
    "LR=0.001\n",
    "TRAIN_ITERS=400\n",
    "BATCH_SIZE=128\n",
    "DISP_STEPS=200\n",
    "DROPOUT=0.5\n",
    "REGULARIZATION_RATE=0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.nn.softmax_cross_entropy_with_logits(logits, labels, name=None)\n",
    "第一个参数logits：就是神经网络最后一层的输出，如果有batch的话，它的大小就是[batchsize，num_classes]，单样本的话，大小就是num_classes\n",
    "第二个参数labels：实际的标签，大小同上\n",
    "\n",
    "__注意！！！__ 这个函数的返回值并不是一个数，而是一个向量，如果要求交叉熵，我们要再做一步tf.reduce_sum操作,就是对向量里面所有元素求和，最后才得到.\n",
    "\n",
    "### tf.argmax(x,axis)\n",
    "返回在axis轴方向上，最大的值对应的下标，和np.argmax一样\n",
    "```\n",
    "test = np.array([[1, 2, 3], \n",
    "                [2, 3, 4], \n",
    "                [5, 4, 3], \n",
    "                [8, 7, 2]])\n",
    "np.argmax(test, 0)　　　＃输出：往axis=0 轴坍缩，array([3, 3, 1]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提前准备好validation 数据和 test数据\n",
    "x_val=np.reshape(mnist.validation.images, (mnist.validation.num_examples,image_size,image_size,n_channels))\n",
    "y_val=mnist.validation.labels\n",
    "\n",
    "x_test=np.reshape(mnist.test.images, (mnist.test.num_examples,image_size,image_size,n_channels))\n",
    "y_test=mnist.test.labels\n",
    "\n",
    "# print(x_test[0,:,:,0].shape)\n",
    "# print(y_test.shape)\n",
    "# np.argmax(y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用matplot显示, 使用 subplot2grid方式\n",
    "# for i in range(3):\n",
    "#     for j in range(3):\n",
    "#         ax=plt.subplot2grid((3,3),(i,j))\n",
    "#         idx=random.randint(0,mnist.test.num_examples)\n",
    "#         img=x_test[idx,:,:,0]\n",
    "#         ax.imshow(img)\n",
    "#         ax.set_title(np.argmax(y_test[idx]))\n",
    "# 使用subplot方式\n",
    "# 3s3方式\n",
    "# for i in range(9):\n",
    "#     plt.subplot(331+i)\n",
    "#     idx=random.randint(0,mnist.test.num_examples)\n",
    "#     img=x_test[idx,:,:,0]\n",
    "#     # plt.cm.gray_r 和plt.cm.gray刚好相反\n",
    "#     plt.imshow(img,cmap = plt.cm.gray_r)\n",
    "#     plt.axis('off')\n",
    "#     plt.title(np.argmax(y_test[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实验日记\n",
    "#### 2018-4-19\n",
    "- x1,x2,x3=sess.run([.. .. ..], feed_dict={}), 等号左边的变量千万不要和[]的变量重合\n",
    "- sess.run是仅仅计算输入的tensor，如果没有把optimizer加进来，则不会进行优化过程\n",
    "- 注意这里tf.nn.softmax_cross_entropy_with_logits(logits, labels, name=None)， labels的输入和logits不一样，logits直接输入为一个M×N的tensor，M为batch size， N为class的数量， 而labels则是实实在在的标签，输入的应该是M维的向量\n",
    "#### 2018-4-20\n",
    "- restore最新的checkpoint\n",
    "如果是使用 saver.save(sess, model_name)\n",
    "则恢复的时候用 saver.restore(sess, model_name)即可\n",
    "\n",
    "如果要加入step作为时间戳\n",
    "则应该用 saver.save(sess, model_name, global_step=step)\n",
    "读取的过程稍稍麻烦（但通用，上一种也能用）\n",
    "ckpt = tf.train.get_checkpoint_state(checkpoint_dir)  \n",
    "if ckpt and ckpt.model_checkpoint_path:  \n",
    "   saver.restore(sess, ckpt.model_checkpoint_path)  \n",
    "else:  \n",
    "   print(\"Not found\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 训练函数\n",
    "def LeNet5train():\n",
    "    x = tf.placeholder(tf.float32, [None, image_size,\n",
    "                                    image_size,\n",
    "                                    n_channels], name='x-input')\n",
    "    y_ = tf.placeholder(tf.float32, [None, n_classes], name='y-input')\n",
    "\n",
    "    \n",
    "    regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)\n",
    "    y=LeNet5(x,False,regularizer)\n",
    "    cross_entropy_mean=tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y,labels=tf.argmax(y_, 1)))\n",
    "    loss=cross_entropy_mean+tf.add_n(tf.get_collection('losses'))\n",
    "    tf.summary.scalar('trainloss',loss) \n",
    "    #正确率\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "    acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    #定义一个global_steps作为全局的time stamp\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    #优化\n",
    "    optimizer=tf.train.GradientDescentOptimizer(LR).minimize(loss, global_step=global_step)\n",
    "    #合并到Summary中  \n",
    "    merged = tf.summary.merge_all() \n",
    "    #调用sess\n",
    "    saver=tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        #选定可视化存储目录  \n",
    "        twriter = tf.summary.FileWriter(base_path+\"/logs/t\", sess.graph)\n",
    "        vwriter=tf.summary.FileWriter(base_path+\"/logs/v\")\n",
    "        #初始化所有变量\n",
    "        tf.global_variables_initializer().run()\n",
    "#         try:\n",
    "#             saver.restore(sess, os.path.join(MODEL_SAVE_PATH,MODEL_NAME))\n",
    "#         except:\n",
    "#             print(\"model not found\")\n",
    "        ckpt = tf.train.get_checkpoint_state(MODEL_SAVE_PATH)  \n",
    "        if ckpt and ckpt.model_checkpoint_path:  \n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            print(\"Restore \"+ckpt.model_checkpoint_path)\n",
    "        else:  \n",
    "            print(\"Model not found, train a new one\")\n",
    "        for i in range(TRAIN_ITERS):\n",
    "            xs, ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "            #不知道这里拿出来的xs的维数是怎么样的，按照教程，应该是1x784，所以在本网络中要reshape一下\n",
    "            x_rs=np.reshape(xs,(BATCH_SIZE,image_size,image_size,n_channels))\n",
    "            \n",
    "            rs1, _, acc_value,  loss_value, stepsss = sess.run([merged,optimizer, acc, loss, global_step],feed_dict={x: x_rs, y_: ys})\n",
    "            if i%DISP_STEPS==0:\n",
    "                twriter.add_summary(rs1, stepsss)\n",
    "                #val精度\n",
    "                rs2, val_acc_value=sess.run([merged,acc], feed_dict={x:x_val,y_:y_val})\n",
    "                vwriter.add_summary(rs2, stepsss)\n",
    "                print(\"step %d: loss: %g  acc: %g  val_acc: %g\" %(stepsss, loss_value,acc_value,val_acc_value))\n",
    "        saver.save(sess,os.path.join(MODEL_SAVE_PATH,MODEL_NAME),global_step=global_step)\n",
    "#         saver.save(sess,os.path.join(MODEL_SAVE_PATH,MODEL_NAME))\n",
    "        # test acc\n",
    "        test_acc=sess.run(acc, feed_dict={x:x_test,y_:y_test})\n",
    "        print(\"final test acc: %g\"%test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/MNIST.ckpt-400\n",
      "Restore ./model/MNIST.ckpt-400\n",
      "step 401: loss: 1.34506  acc: 0.804688  val_acc: 0.8228\n",
      "step 601: loss: 1.15028  acc: 0.8125  val_acc: 0.862\n",
      "final test acc: 0.8905\n"
     ]
    }
   ],
   "source": [
    "LeNet5train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
